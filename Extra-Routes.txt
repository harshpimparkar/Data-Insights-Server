from server.QueryHelper.CSVtoEmbedding import EmbeddingGenerator
from StoreEmbeddings import store_embeddings
from QuerytoEmbeddings import query_to_embedding
chat_to_vis_service = ChatVisualizationService()
emb_service = EmbeddingGenerator()
Load environment variables

# Get Pinecone configuration
PINECONE_API_KEY = os.getenv("PINECONE_API_KEY")
PINECONE_INDEX_NAME = os.getenv("PINECONE_INDEX")
if not PINECONE_API_KEY or not PINECONE_INDEX_NAME:
    raise EnvironmentError("Pinecone API key or index name is not set.")

# Initialize Pinecone client
pc = Pinecone(api_key=PINECONE_API_KEY)
index = pc.Index(PINECONE_INDEX_NAME)


@app.route('/v1/chat-extended', methods=['POST'])
def chat_with_csv():
    """
    Endpoint to handle chat requests using semantic search on CSV embeddings.
    """
    try:
        chat_request = request.get_json()
        query = chat_request.get("query", "").strip()

        if not query:
            return jsonify({"status": "fail", "message": "Query text is required."}), 400

        # Retrieve namespace from the stored map
        namespace = namespace_map.get("current_namespace")
        if not namespace:
            return jsonify({"status": "fail", "message": "No namespace available. Upload a file first."}), 400

        logging.info(f"Processing query: '{query}' for namespace: '{namespace}'")

        # Generate embedding for the query
        query_result = query_to_embedding(query)
        if query_result["status"] == "fail":
            return jsonify(query_result), 500

        query_embedding = query_result["embedding"]

        # Perform semantic search with the stored namespace
        search_result = search_embeddings(query_embedding, namespace=namespace)
        if search_result["status"] == "fail":
            return jsonify(search_result), 500

        # Construct the response
        return jsonify({
            "status": "success",
            "query": query,
            "namespace": namespace,
            "results": search_result["results"]
        }), 200

    except Exception as e:
        logging.error(f"Error processing chat request: {e}")
        return jsonify({"status": "fail", "message": "Failed to process the chat request."}), 500

@app.route('/v1/upload-csv-file-extended', methods=['POST'])
def upload_csv_extended():
    """Endpoint to upload a CSV file and generate/store embeddings."""
    if 'file' not in request.files:
        return jsonify({"status": "fail", "message": "No file part"}), 400

    file = request.files['file']
    if not file.filename.endswith('.csv'):
        return jsonify({"status": "fail", "message": "Only CSV files are allowed"}), 400

    file_path = os.path.join(app.config['UPLOAD_FOLDER'], file.filename)
    file.save(file_path)
    logging.info(f"File '{file.filename}' uploaded successfully.")

    # Generate embeddings
    result = generate_embeddings(file_path)
    if result["status"] == "fail":
        logging.error(f"Embedding generation failed for '{file.filename}': {result['message']}")
        return jsonify({"status": "fail", "message": result["message"]}), 500

    # Store embeddings with namespace
    namespace = os.path.splitext(file.filename)[0]  # Use filename (without extension) as namespace
    store_result = store_embeddings(result["embeddings"], namespace=namespace)
    if store_result["status"] == "fail":
        logging.error(f"Failed to store embeddings for '{file.filename}': {store_result['message']}")
        return jsonify({"status": "fail", "message": store_result["message"]}), 500

    # Store namespace in the global map
    namespace_map["current_namespace"] = namespace

    logging.info(f"Embeddings from '{file.filename}' stored successfully under namespace '{namespace}'.")

    return jsonify({
        "status": "success",
        "message": "File uploaded, embeddings generated, and stored successfully.",
        "embedding_details": result["embedding_details"],
        "namespace": namespace
    }), 200

    # def search_embeddings(query_embedding, top_k=5, namespace=None):

# Upload CSV Route:
@app.route('/v1/upload-csv-file', methods=['POST'])
def upload_csv():
    """Endpoint to upload a CSV file, generate embeddings, and store them in Pinecone."""
    if 'file' not in request.files:
        return jsonify({"status": "fail", "message": "No file part in the request"}), 400

    file = request.files['file']
    if not file.filename:
        return jsonify({"status": "fail", "message": "No file selected"}), 400

    if not file.filename.endswith('.csv'):
        return jsonify({"status": "fail", "message": "Only CSV files are allowed"}), 400

    try:
        # Create upload folder if it doesn't exist
        os.makedirs(app.config['UPLOAD_FOLDER'], exist_ok=True)
        
        # Save the uploaded file with secure filename
        filename = secure_filename(file.filename)
        file_path = os.path.join(app.config['UPLOAD_FOLDER'], filename)
        file.save(file_path)
        
        # Generate embeddings
        result = generate_embeddings(file_path)
        if result["status"] == "fail":
            return jsonify({"status": "fail", "message": result["message"]}), 500

        pinecone_data = result.get("pinecone_data", [])
        if not pinecone_data:
            return jsonify({"status": "fail", "message": "No embeddings generated"}), 500

        # Use filename as namespace
        namespace = os.path.splitext(filename)[0]
        
        # Store embeddings in Pinecone
        store_result = store_embeddings(embeddings=pinecone_data, namespace=namespace)
        if store_result["status"] == "fail":
            return jsonify({"status": "fail", "message": store_result["message"]}), 500

        # Update current namespace after successful upload
        update_current_namespace(namespace, filename)

        # Clean up
        try:
            os.remove(file_path)
        except Exception as e:
            logging.warning(f"Failed to clean up file {file_path}: {str(e)}")

        return jsonify({
            "status": "success",
            "message": "File processed successfully",
            "details": {
                "namespace": namespace,
                "num_embeddings": store_result["num_embeddings"],
                "file_name": filename
            },
            "embeddings": pinecone_data,
        }), 200

    except Exception as e:
        logging.error(f"Error processing file: {str(e)}", exc_info=True)
        return jsonify({"status": "fail", "message": str(e)}), 500

# Test GROQ Chat Route:
@app.route('/v1/chat', methods=['POST']) 
def chat_with_llm():     
    """     Endpoint to handle chat requests using Groq LLM API.     """
    try:      
        chat_request = request.get_json()         
        response = initialize_groq_api(chat_request)         
        return jsonify(response), 200     
    except Exception as e:         
        logging.error(f"Groq API error: {e}")         
        return jsonify({"error": "Failed to process the chat request."}), 500  

# FOR QUERYING PINECONE INDEX:
@app.route("/v1/query", methods=["POST"])
def query():
    """Query endpoint that uses the most recently uploaded file's namespace and generates LLM response"""
    try:
        # Check if we have an active namespace
        if not current_namespace["name"]:
            return jsonify({
                "status": "fail", 
                "message": "No file has been uploaded yet. Please upload a CSV file first."
            }), 400

        # Extract query from the POST request
        data = request.json
        if not data or "query" not in data:
            return jsonify({"status": "fail", "message": "Query is required."}), 400

        query = data.get("query")
        top_k = data.get("top_k", 10)  # Increased default to get more context

        # Convert query to embedding
        embedding_response = query_to_embedding(query)
        if embedding_response.get("status") != "success":
            return jsonify(embedding_response), 400

        query_embedding = embedding_response.get("embedding")
        if not query_embedding:
            return jsonify({"status": "fail", "message": "Failed to generate embedding."}), 500

        # Perform search using current namespace
        results = index.query(
            namespace=current_namespace["name"],
            vector=query_embedding,
            top_k=top_k,
            include_values=False,
            include_metadata=True
        )

        # Process the results to ensure they're JSON serializable
        processed_matches = []
        if results and hasattr(results, 'matches'):
            for match in results.matches:
                processed_match = {
                    "id": match.id if hasattr(match, 'id') else None,
                    "score": float(match.score) if hasattr(match, 'score') else None,
                    "metadata": dict(match.metadata) if hasattr(match, 'metadata') and match.metadata else {}
                }
                processed_matches.append(processed_match)

        # Format context from matches
        context = format_context_from_matches(processed_matches, query)
        
        # Generate LLM prompt
        llm_prompt = generate_llm_prompt(query, context)
        
        # Get LLM response
        llm_response = initialize_groq_api(llm_prompt)

        return jsonify({
            "status": "success",
            "query": query,
            "file_details": {
                "file_name": current_namespace["file_name"],
                "uploaded_at": current_namespace["timestamp"]
            },
            "llm_response": llm_response,
            "context": {
                "matches": processed_matches,
                "formatted_context": context
            }
        }), 200

    except Exception as e:
        logging.error(f"Error in query route: {str(e)}", exc_info=True)
        logging.error("Full traceback:", exc_info=True)
        return jsonify({
            "status": "fail", 
            "message": str(e),
            "type": str(type(e).__name__)
        }), 500

# Check Current File Route:
@app.route("/v1/current-file", methods=["GET"])
def get_current_file():
    """Endpoint to check which file is currently active for querying"""
    if not current_namespace["name"]:
        return jsonify({
            "status": "fail",
            "message": "No file is currently active. Please upload a CSV file."
        }), 404

    return jsonify({
        "status": "success",
        "current_file": {
            "file_name": current_namespace["file_name"],
            "namespace": current_namespace["name"],
            "uploaded_at": current_namespace["timestamp"]
        }
    })



@app.route('/v1/upload-to-insights', methods=['POST'])
def upload_and_generate_insights():
    try:
        logging.info("Request received at /v1/upload-to-insights")

        # Handle file upload
        file = request.files.get('file')
        target_column = request.form.get('target_column', None)
        generate_llm_analysis = request.form.get('generate_llm_analysis', 'true').lower() == 'true'
        
        logging.info(f"Processing file with target_column: {target_column}, LLM analysis: {generate_llm_analysis}")

        if not file or not file.filename:
            logging.warning("No file provided or filename is empty")
            return jsonify({"error": "No file provided or filename is empty"}), 400

        # Validate file type
        if not file.filename.endswith('.csv'):
            logging.warning(f"Invalid file type: {file.filename}. Only CSV files are allowed.")
            return jsonify({"error": "Only CSV files are allowed"}), 400

        # Save file
        try:
            os.makedirs('uploads', exist_ok=True)
            file_path = os.path.join('uploads', secure_filename(file.filename))
            file.save(file_path)
            logging.info(f"File uploaded successfully: {file_path}")
        except Exception as e:
            logging.error(f"Error saving file: {e}")
            return jsonify({"error": "Failed to save uploaded file", "details": str(e)}), 500

        # Initialize insights generator
        try:
            generator = DataInsightsGenerator(file_path)
            logging.info("DataInsightsGenerator initialized successfully")
        except Exception as e:
            logging.error(f"Error initializing DataInsightsGenerator: {e}")
            return jsonify({"error": "Failed to process CSV file", "details": str(e)}), 422

        # Generate insights
        insights = {
            "descriptive": {},
            "diagnostic": {},
            "predictive": {},
            "prescriptive": {},
            "outliers": {}
        }

        # Generate analytics with proper error handling
        analytics_functions = {
            "descriptive": generator.generate_descriptive_analytics,
            "diagnostic": generator.generate_diagnostic_analytics,
            "prescriptive": generator.generate_prescriptive_analytics,
            "outliers": generator.detect_outliers
        }

        for insight_type, func in analytics_functions.items():
            try:
                result = func()
                insights[insight_type] = generator._convert_to_serializable(result)
                logging.info(f"{insight_type.capitalize()} analytics generated successfully")
            except Exception as e:
                logging.error(f"Error generating {insight_type} analytics: {e}")
                insights[insight_type] = {"error": str(e)}

        # Handle predictive analytics
        if target_column:
            try:
                result = generator.generate_predictive_analytics(target_column)
                insights["predictive"] = generator._convert_to_serializable(result)
                logging.info("Predictive analytics generated successfully")
            except Exception as e:
                logging.error(f"Error generating predictive analytics: {e}")
                insights["predictive"] = {"error": str(e)}

        response_data = {
            "message": "File uploaded and insights generated successfully",
            "file_path": file_path,
            "insights": insights
        }

        # Generate LLM analysis if requested
        if generate_llm_analysis:
            try:
                logging.info("Generating LLM analysis")
                insights_context = format_insights_for_llm(insights)
                llm_prompt = generate_insight_llm_prompt(insights_context)
                llm_response = initialize_groq_api(llm_prompt)
                response_data["llm_analysis"] = llm_response
                logging.info("LLM analysis generated successfully")
            except Exception as e:
                logging.error(f"Error generating LLM analysis: {e}")
                response_data["llm_analysis"] = {"error": str(e)}

        # Ensure JSON serialization
        try:
            return jsonify(response_data), 200
        except TypeError as e:
            logging.error(f"JSON serialization error: {e}")
            clean_response = generator._convert_to_serializable(response_data)
            return jsonify(clean_response), 200

    except Exception as e:
        logging.error(f"Error in /v1/upload-to-insights route: {e}", exc_info=True)
        return jsonify({
            "error": "Failed to process request",
            "details": str(e)
        }), 500

    finally:
        try:
            if 'file_path' in locals() and os.path.exists(file_path):
                # Uncomment to delete uploaded file after processing
                # os.remove(file_path)
                pass
        except Exception as e:
            logging.error(f"Error in cleanup: {e}")

# TESTING VISUALIZATION GENERATION:

@app.route('/v1/test', methods=['GET'])
def test_end():
    """Endpoint to test the API."""
    return jsonify({
        'status': 'success',
        'message': 'Weapons Hot!'
    }), 200


@app.route('/v1/heavy-file-status', methods=['GET'])
def get_file_status():
    """Get current file status and available operations"""
    if not current_file["path"]:
        return jsonify({
            "status": "no_file",
            "message": "No file currently uploaded"
        }), 404

    return jsonify({
        "status": "success",
        "file_details": {
            "name": current_file["name"],
            "uploaded_at": current_file["timestamp"],
            "operations": {
                "has_embeddings": current_file["has_embeddings"],
                "has_insights": current_file["has_insights"]
            }
        }
    })



# Deprecated Route do not use: 
@app.route('/v1/csv-query', methods=['POST'])
def process_and_query():
    """Combined endpoint to process embeddings and query in one step"""
    try:
        logging.info("Received heavy-query request")
        logging.debug(f"Request JSON: {request.json}")
        
        # Check if file exists
        if not current_file.get("path"):
            logging.error("No file uploaded before query attempt")
            return jsonify({
                "status": "fail", 
                "message": "No file uploaded. Please upload a file first.",
                "error_code": "NO_FILE"
            }), 400

        # Validate request JSON
        if not request.is_json:
            logging.error("Request missing JSON payload")
            return jsonify({
                "status": "fail",
                "message": "Request must include JSON payload",
                "error_code": "INVALID_REQUEST"
            }), 400

        data = request.json
        if not data:
            logging.error("Empty JSON payload received")
            return jsonify({
                "status": "fail",
                "message": "Request body cannot be empty",
                "error_code": "EMPTY_PAYLOAD"
            }), 400

        if "query" not in data:
            logging.error("Query parameter missing from request")
            return jsonify({
                "status": "fail",
                "message": "Query parameter is required",
                "error_code": "MISSING_QUERY"
            }), 400

        query = data["query"]
        if not isinstance(query, str) or not query.strip():
            logging.error(f"Invalid query format or empty query: {query}")
            return jsonify({
                "status": "fail",
                "message": "Query must be a non-empty string",
                "error_code": "INVALID_QUERY"
            }), 400

        # Step 1: Generate and store embeddings if not already done
        if not current_file["has_embeddings"]:
            logging.info(f"Generating embeddings for file: {current_file['name']}")
            
            result = emb_service.generate_embeddings(current_file["path"])
            if result["status"] != "success":
                logging.error(f"Embedding generation failed: {result.get('message', 'Unknown error')}")
                return jsonify(result), 500

            # Fix: Use the 'data' key instead of 'pinecone_data'
            embeddings_data = result.get("data", [])
            if not embeddings_data:
                logging.error("No embeddings generated from file")
                return jsonify({
                    "status": "fail",
                    "message": "No embeddings generated",
                    "error_code": "NO_EMBEDDINGS"
                }), 500

            # Store embeddings with proper data structure
            store_result = store_embeddings(
                embeddings=embeddings_data,
                namespace=current_file["namespace"]
            )
            
            if store_result["status"] != "success":
                logging.error(f"Failed to store embeddings: {store_result.get('message', 'Unknown error')}")
                return jsonify(store_result), 500

            current_file["has_embeddings"] = True
            logging.info(f"Successfully stored {store_result.get('num_embeddings', 0)} embeddings")

        # Step 2: Process query and get results
        logging.info(f"Processing query: {query}")
        embedding_response = query_to_embedding(query)
        if embedding_response.get("status") != "success":
            logging.error(f"Query embedding failed: {embedding_response.get('message', 'Unknown error')}")
            return jsonify(embedding_response), 400

        query_embedding = embedding_response.get("embedding")
        if not query_embedding:
            logging.error("No embedding generated for query")
            return jsonify({
                "status": "fail",
                "message": "Failed to generate query embedding",
                "error_code": "NO_QUERY_EMBEDDING"
            }), 500

        try:
            results = index.query(
                namespace=current_file["namespace"],
                vector=query_embedding,
                top_k=25,
                include_values=False,
                include_metadata=True
            )
        except Exception as e:
            logging.error(f"Pinecone query failed: {str(e)}", exc_info=True)
            return jsonify({
                "status": "fail",
                "message": "Vector database query failed",
                "error_code": "VECTOR_QUERY_FAILED",
                "error_details": str(e)
            }), 500

        # Process matches with improved error handling
        processed_matches = []
        if hasattr(results, 'matches'):
            for i, match in enumerate(results.matches):
                try:
                    processed_match = {
                        "id": str(match.id),
                        "score": float(match.score) if hasattr(match, 'score') else 0.0,
                        "metadata": dict(match.metadata) if hasattr(match, 'metadata') and match.metadata is not None else {}
                    }
                    processed_matches.append(processed_match)
                except Exception as e:
                    logging.error(f"Error processing match {i}: {str(e)}", exc_info=True)
                    continue

        if not processed_matches:
            return jsonify({
                "status": "success",
                "message": "No relevant matches found",
                "file_details": {
                    "name": current_file["name"],
                    "namespace": current_file["namespace"]
                },
                "embeddings_status": {
                    "was_generated": not current_file["has_embeddings"]
                },
                "query_results": {
                    "query": query,
                    "matches": []
                }
            }), 200

        context = format_context_from_matches(processed_matches, query)
        llm_prompt = generate_llm_prompt(query, context)
        llm_response = initialize_groq_api(llm_prompt)

        if llm_response is None:
            logging.error("Failed to get LLM response")
            llm_response = {"error": "Failed to get LLM response"}
        elif isinstance(llm_response, str):
            llm_response = {"response": llm_response}

        response_data = {
            "status": "success",
            "file_details": {
                "name": current_file["name"],
                "namespace": current_file["namespace"]
            },
            "embeddings_status": {
                "was_generated": not current_file["has_embeddings"],
                "message": "Embeddings were generated and stored"
            },
            "query_results": {
                "query": query,
                "llm_response": llm_response,
                "matches": processed_matches
            }
        }

        logging.info("Query processed successfully")
        return jsonify(response_data), 200

    except Exception as e:
        logging.error(f"Unexpected error processing query: {str(e)}", exc_info=True)
        return jsonify({
            "status": "fail",
            "message": "Query processing failed",
            "error_code": "UNEXPECTED_ERROR",
            "error_details": str(e)
        }), 500
        
@app.route('/v1/heavy-insights', methods=['GET'])
def generate_insights():
    """Generate insights and visualizations from the uploaded file when requested via GET"""
    try:
        # Validate file existence
        if not current_file.get("path"):
            return jsonify({
                "status": "fail",
                "message": "No file uploaded. Please upload a file first."
            }), 400

        # Validate file readability and DataFrame structure
        try:
            generator = DataInsightsGenerator(current_file["path"])
            if generator.df.empty:
                return jsonify({
                    "status": "fail",
                    "message": "The uploaded file contains no data."
                }), 400
            if len(generator.df.columns) == 0:
                return jsonify({
                    "status": "fail",
                    "message": "The uploaded file contains no columns."
                }), 400
        except Exception as e:
            return jsonify({
                "status": "fail",
                "message": f"Failed to read the uploaded file: {str(e)}"
            }), 400

        # Get optional query parameters
        target_column = request.args.get('target_column')
        generate_llm_analysis = request.args.get('generate_llm_analysis', 'true').lower() == 'true'

        logging.info(f"Generating insights for file: {current_file['name']}")
        
        # Initialize insights dictionary with safe defaults
        insights = {}
        
        # Generate each type of insight with individual try-except blocks
        try:
            insights["descriptive"] = generator.generate_descriptive_analytics()
        except Exception as e:
            logging.warning(f"Failed to generate descriptive analytics: {str(e)}")
            insights["descriptive"] = {"error": str(e)}

        try:
            insights["diagnostic"] = generator.generate_diagnostic_analytics()
        except Exception as e:
            logging.warning(f"Failed to generate diagnostic analytics: {str(e)}")
            insights["diagnostic"] = {"error": str(e)}

        try:
            insights["prescriptive"] = generator.generate_prescriptive_analytics()
        except Exception as e:
            logging.warning(f"Failed to generate prescriptive analytics: {str(e)}")
            insights["prescriptive"] = {"error": str(e)}

        try:
            insights["outliers"] = generator.detect_outliers()
        except Exception as e:
            logging.warning(f"Failed to detect outliers: {str(e)}")
            insights["outliers"] = {"error": str(e)}

        # Generate predictive analytics if target column is provided
        if target_column:
            if target_column not in generator.df.columns:
                insights["predictive"] = {
                    "error": f"Target column '{target_column}' not found in dataset"
                }
            else:
                try:
                    insights["predictive"] = generator.generate_predictive_analytics(target_column)
                except Exception as e:
                    logging.warning(f"Failed to generate predictive analytics: {str(e)}")
                    insights["predictive"] = {"error": str(e)}

        # Convert insights to serializable format with error handling
        try:
            insights = generator._convert_to_serializable(insights)
        except Exception as e:
            logging.error(f"Failed to convert insights to serializable format: {str(e)}")
            return jsonify({
                "status": "fail",
                "message": "Failed to process generated insights"
            }), 500

        response_data = {
            "status": "success",
            "file_details": {
                "name": current_file["name"],
                "path": current_file["path"]
            },
            "insights": insights
        }

        # Generate LLM analysis and visualization recommendations if requested
        if generate_llm_analysis:
            logging.info("Generating LLM analysis and visualization recommendations")
            try:
                insights_context = format_insights_for_llm(insights)
                llm_prompt = generate_insight_llm_prompt(insights_context)
                llm_response = initialize_groq_api(llm_prompt)
                response_data["llm_analysis"] = llm_response
                
                # Generate visualization recommendations from LLM
                viz_prompt = f"""
                Based on the data analysis and insights provided, recommend appropriate visualizations.
                For each visualization, provide:
                1. Chart type (choose from: line, bar, scatter, area)
                2. X-axis column
                3. Y-axis column
                4. Title
                5. Brief description of what the visualization shows
                
                Data columns: {', '.join(generator.df.columns)}
                
                Format your response as:
                {{
                    "visualizations": [
                        {{
                            "type": "chart_type",
                            "x_column": "column_name",
                            "y_column": "column_name",
                            "title": "chart_title",
                            "description": "brief_description"
                        }},
                        ...
                    ]
                }}
                """
                
                viz_recommendations = initialize_groq_api(viz_prompt)
                
                # Generate the visualizations
                try:
                    viz_data = []
                    for viz in viz_recommendations.get('visualizations', []):
                        chart_data = generator.df[[viz['x_column'], viz['y_column']]].dropna()
                        chart_data = chart_data.to_dict('records')
                        
                        viz_data.append({
                            'spec': {
                                'plot_type': viz['type'],
                                'x_column': viz['x_column'],
                                'y_column': viz['y_column'],
                                'title': viz['title']
                            },
                            'description': viz['description'],
                            'data': chart_data
                        })
                    
                    response_data["visualizations"] = viz_data
                    
                except Exception as e:
                    logging.error(f"Failed to generate visualizations: {str(e)}")
                    response_data["visualizations"] = {"error": str(e)}
                
            except Exception as e:
                logging.error(f"Failed to generate LLM analysis: {str(e)}")
                response_data["llm_analysis"] = {"error": str(e)}
                response_data["visualizations"] = {"error": str(e)}

        # Update file state
        current_file["has_insights"] = True
        current_file["last_insights"] = insights

        logging.info("Successfully generated insights and visualizations")
        return jsonify(response_data), 200

    except Exception as e:
        logging.error(f"Error generating insights: {str(e)}", exc_info=True)
        return jsonify({
            "status": "fail",
            "message": str(e),
            "type": str(type(e).__name__),
            "details": "An unexpected error occurred while generating insights"
        }), 500