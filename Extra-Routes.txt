@app.route('/v1/chat-extended', methods=['POST'])
def chat_with_csv():
    """
    Endpoint to handle chat requests using semantic search on CSV embeddings.
    """
    try:
        chat_request = request.get_json()
        query = chat_request.get("query", "").strip()

        if not query:
            return jsonify({"status": "fail", "message": "Query text is required."}), 400

        # Retrieve namespace from the stored map
        namespace = namespace_map.get("current_namespace")
        if not namespace:
            return jsonify({"status": "fail", "message": "No namespace available. Upload a file first."}), 400

        logging.info(f"Processing query: '{query}' for namespace: '{namespace}'")

        # Generate embedding for the query
        query_result = query_to_embedding(query)
        if query_result["status"] == "fail":
            return jsonify(query_result), 500

        query_embedding = query_result["embedding"]

        # Perform semantic search with the stored namespace
        search_result = search_embeddings(query_embedding, namespace=namespace)
        if search_result["status"] == "fail":
            return jsonify(search_result), 500

        # Construct the response
        return jsonify({
            "status": "success",
            "query": query,
            "namespace": namespace,
            "results": search_result["results"]
        }), 200

    except Exception as e:
        logging.error(f"Error processing chat request: {e}")
        return jsonify({"status": "fail", "message": "Failed to process the chat request."}), 500

@app.route('/v1/upload-csv-file-extended', methods=['POST'])
def upload_csv_extended():
    """Endpoint to upload a CSV file and generate/store embeddings."""
    if 'file' not in request.files:
        return jsonify({"status": "fail", "message": "No file part"}), 400

    file = request.files['file']
    if not file.filename.endswith('.csv'):
        return jsonify({"status": "fail", "message": "Only CSV files are allowed"}), 400

    file_path = os.path.join(app.config['UPLOAD_FOLDER'], file.filename)
    file.save(file_path)
    logging.info(f"File '{file.filename}' uploaded successfully.")

    # Generate embeddings
    result = generate_embeddings(file_path)
    if result["status"] == "fail":
        logging.error(f"Embedding generation failed for '{file.filename}': {result['message']}")
        return jsonify({"status": "fail", "message": result["message"]}), 500

    # Store embeddings with namespace
    namespace = os.path.splitext(file.filename)[0]  # Use filename (without extension) as namespace
    store_result = store_embeddings(result["embeddings"], namespace=namespace)
    if store_result["status"] == "fail":
        logging.error(f"Failed to store embeddings for '{file.filename}': {store_result['message']}")
        return jsonify({"status": "fail", "message": store_result["message"]}), 500

    # Store namespace in the global map
    namespace_map["current_namespace"] = namespace

    logging.info(f"Embeddings from '{file.filename}' stored successfully under namespace '{namespace}'.")

    return jsonify({
        "status": "success",
        "message": "File uploaded, embeddings generated, and stored successfully.",
        "embedding_details": result["embedding_details"],
        "namespace": namespace
    }), 200

    # def search_embeddings(query_embedding, top_k=5, namespace=None):
#     """
#     Perform semantic search on the Pinecone index using the query embedding.
#     """
#     try:
#         results = index.query(
#             vector=query_embedding,
#             top_k=top_k,
#             namespace=namespace,
#             include_metadata=True
#         )
#         return {"status": "success", "results": results.matches}
#     except Exception as e:
#         logging.error(f"Error performing semantic search: {e}")
#         return {"status": "fail", "message": str(e)}

# Upload CSV Route:
@app.route('/v1/upload-csv-file', methods=['POST'])
def upload_csv():
    """Endpoint to upload a CSV file, generate embeddings, and store them in Pinecone."""
    if 'file' not in request.files:
        return jsonify({"status": "fail", "message": "No file part in the request"}), 400

    file = request.files['file']
    if not file.filename:
        return jsonify({"status": "fail", "message": "No file selected"}), 400

    if not file.filename.endswith('.csv'):
        return jsonify({"status": "fail", "message": "Only CSV files are allowed"}), 400

    try:
        # Create upload folder if it doesn't exist
        os.makedirs(app.config['UPLOAD_FOLDER'], exist_ok=True)
        
        # Save the uploaded file with secure filename
        filename = secure_filename(file.filename)
        file_path = os.path.join(app.config['UPLOAD_FOLDER'], filename)
        file.save(file_path)
        
        # Generate embeddings
        result = generate_embeddings(file_path)
        if result["status"] == "fail":
            return jsonify({"status": "fail", "message": result["message"]}), 500

        pinecone_data = result.get("pinecone_data", [])
        if not pinecone_data:
            return jsonify({"status": "fail", "message": "No embeddings generated"}), 500

        # Use filename as namespace
        namespace = os.path.splitext(filename)[0]
        
        # Store embeddings in Pinecone
        store_result = store_embeddings(embeddings=pinecone_data, namespace=namespace)
        if store_result["status"] == "fail":
            return jsonify({"status": "fail", "message": store_result["message"]}), 500

        # Update current namespace after successful upload
        update_current_namespace(namespace, filename)

        # Clean up
        try:
            os.remove(file_path)
        except Exception as e:
            logging.warning(f"Failed to clean up file {file_path}: {str(e)}")

        return jsonify({
            "status": "success",
            "message": "File processed successfully",
            "details": {
                "namespace": namespace,
                "num_embeddings": store_result["num_embeddings"],
                "file_name": filename
            },
            "embeddings": pinecone_data,
        }), 200

    except Exception as e:
        logging.error(f"Error processing file: {str(e)}", exc_info=True)
        return jsonify({"status": "fail", "message": str(e)}), 500

# Test GROQ Chat Route:
@app.route('/v1/chat', methods=['POST']) 
def chat_with_llm():     
    """     Endpoint to handle chat requests using Groq LLM API.     """
    try:      
        chat_request = request.get_json()         
        response = initialize_groq_api(chat_request)         
        return jsonify(response), 200     
    except Exception as e:         
        logging.error(f"Groq API error: {e}")         
        return jsonify({"error": "Failed to process the chat request."}), 500  

# FOR QUERYING PINECONE INDEX:
@app.route("/v1/query", methods=["POST"])
def query():
    """Query endpoint that uses the most recently uploaded file's namespace and generates LLM response"""
    try:
        # Check if we have an active namespace
        if not current_namespace["name"]:
            return jsonify({
                "status": "fail", 
                "message": "No file has been uploaded yet. Please upload a CSV file first."
            }), 400

        # Extract query from the POST request
        data = request.json
        if not data or "query" not in data:
            return jsonify({"status": "fail", "message": "Query is required."}), 400

        query = data.get("query")
        top_k = data.get("top_k", 10)  # Increased default to get more context

        # Convert query to embedding
        embedding_response = query_to_embedding(query)
        if embedding_response.get("status") != "success":
            return jsonify(embedding_response), 400

        query_embedding = embedding_response.get("embedding")
        if not query_embedding:
            return jsonify({"status": "fail", "message": "Failed to generate embedding."}), 500

        # Perform search using current namespace
        results = index.query(
            namespace=current_namespace["name"],
            vector=query_embedding,
            top_k=top_k,
            include_values=False,
            include_metadata=True
        )

        # Process the results to ensure they're JSON serializable
        processed_matches = []
        if results and hasattr(results, 'matches'):
            for match in results.matches:
                processed_match = {
                    "id": match.id if hasattr(match, 'id') else None,
                    "score": float(match.score) if hasattr(match, 'score') else None,
                    "metadata": dict(match.metadata) if hasattr(match, 'metadata') and match.metadata else {}
                }
                processed_matches.append(processed_match)

        # Format context from matches
        context = format_context_from_matches(processed_matches, query)
        
        # Generate LLM prompt
        llm_prompt = generate_llm_prompt(query, context)
        
        # Get LLM response
        llm_response = initialize_groq_api(llm_prompt)

        return jsonify({
            "status": "success",
            "query": query,
            "file_details": {
                "file_name": current_namespace["file_name"],
                "uploaded_at": current_namespace["timestamp"]
            },
            "llm_response": llm_response,
            "context": {
                "matches": processed_matches,
                "formatted_context": context
            }
        }), 200

    except Exception as e:
        logging.error(f"Error in query route: {str(e)}", exc_info=True)
        logging.error("Full traceback:", exc_info=True)
        return jsonify({
            "status": "fail", 
            "message": str(e),
            "type": str(type(e).__name__)
        }), 500

# Check Current File Route:
@app.route("/v1/current-file", methods=["GET"])
def get_current_file():
    """Endpoint to check which file is currently active for querying"""
    if not current_namespace["name"]:
        return jsonify({
            "status": "fail",
            "message": "No file is currently active. Please upload a CSV file."
        }), 404

    return jsonify({
        "status": "success",
        "current_file": {
            "file_name": current_namespace["file_name"],
            "namespace": current_namespace["name"],
            "uploaded_at": current_namespace["timestamp"]
        }
    })



@app.route('/v1/upload-to-insights', methods=['POST'])
def upload_and_generate_insights():
    try:
        logging.info("Request received at /v1/upload-to-insights")

        # Handle file upload
        file = request.files.get('file')
        target_column = request.form.get('target_column', None)
        generate_llm_analysis = request.form.get('generate_llm_analysis', 'true').lower() == 'true'
        
        logging.info(f"Processing file with target_column: {target_column}, LLM analysis: {generate_llm_analysis}")

        if not file or not file.filename:
            logging.warning("No file provided or filename is empty")
            return jsonify({"error": "No file provided or filename is empty"}), 400

        # Validate file type
        if not file.filename.endswith('.csv'):
            logging.warning(f"Invalid file type: {file.filename}. Only CSV files are allowed.")
            return jsonify({"error": "Only CSV files are allowed"}), 400

        # Save file
        try:
            os.makedirs('uploads', exist_ok=True)
            file_path = os.path.join('uploads', secure_filename(file.filename))
            file.save(file_path)
            logging.info(f"File uploaded successfully: {file_path}")
        except Exception as e:
            logging.error(f"Error saving file: {e}")
            return jsonify({"error": "Failed to save uploaded file", "details": str(e)}), 500

        # Initialize insights generator
        try:
            generator = DataInsightsGenerator(file_path)
            logging.info("DataInsightsGenerator initialized successfully")
        except Exception as e:
            logging.error(f"Error initializing DataInsightsGenerator: {e}")
            return jsonify({"error": "Failed to process CSV file", "details": str(e)}), 422

        # Generate insights
        insights = {
            "descriptive": {},
            "diagnostic": {},
            "predictive": {},
            "prescriptive": {},
            "outliers": {}
        }

        # Generate analytics with proper error handling
        analytics_functions = {
            "descriptive": generator.generate_descriptive_analytics,
            "diagnostic": generator.generate_diagnostic_analytics,
            "prescriptive": generator.generate_prescriptive_analytics,
            "outliers": generator.detect_outliers
        }

        for insight_type, func in analytics_functions.items():
            try:
                result = func()
                insights[insight_type] = generator._convert_to_serializable(result)
                logging.info(f"{insight_type.capitalize()} analytics generated successfully")
            except Exception as e:
                logging.error(f"Error generating {insight_type} analytics: {e}")
                insights[insight_type] = {"error": str(e)}

        # Handle predictive analytics
        if target_column:
            try:
                result = generator.generate_predictive_analytics(target_column)
                insights["predictive"] = generator._convert_to_serializable(result)
                logging.info("Predictive analytics generated successfully")
            except Exception as e:
                logging.error(f"Error generating predictive analytics: {e}")
                insights["predictive"] = {"error": str(e)}

        response_data = {
            "message": "File uploaded and insights generated successfully",
            "file_path": file_path,
            "insights": insights
        }

        # Generate LLM analysis if requested
        if generate_llm_analysis:
            try:
                logging.info("Generating LLM analysis")
                insights_context = format_insights_for_llm(insights)
                llm_prompt = generate_insight_llm_prompt(insights_context)
                llm_response = initialize_groq_api(llm_prompt)
                response_data["llm_analysis"] = llm_response
                logging.info("LLM analysis generated successfully")
            except Exception as e:
                logging.error(f"Error generating LLM analysis: {e}")
                response_data["llm_analysis"] = {"error": str(e)}

        # Ensure JSON serialization
        try:
            return jsonify(response_data), 200
        except TypeError as e:
            logging.error(f"JSON serialization error: {e}")
            clean_response = generator._convert_to_serializable(response_data)
            return jsonify(clean_response), 200

    except Exception as e:
        logging.error(f"Error in /v1/upload-to-insights route: {e}", exc_info=True)
        return jsonify({
            "error": "Failed to process request",
            "details": str(e)
        }), 500

    finally:
        try:
            if 'file_path' in locals() and os.path.exists(file_path):
                # Uncomment to delete uploaded file after processing
                # os.remove(file_path)
                pass
        except Exception as e:
            logging.error(f"Error in cleanup: {e}")

# TESTING VISUALIZATION GENERATION:

@app.route('/v1/test', methods=['GET'])
def test_end():
    """Endpoint to test the API."""
    return jsonify({
        'status': 'success',
        'message': 'Weapons Hot!'
    }), 200


@app.route('/v1/heavy-file-status', methods=['GET'])
def get_file_status():
    """Get current file status and available operations"""
    if not current_file["path"]:
        return jsonify({
            "status": "no_file",
            "message": "No file currently uploaded"
        }), 404

    return jsonify({
        "status": "success",
        "file_details": {
            "name": current_file["name"],
            "uploaded_at": current_file["timestamp"],
            "operations": {
                "has_embeddings": current_file["has_embeddings"],
                "has_insights": current_file["has_insights"]
            }
        }
    })
